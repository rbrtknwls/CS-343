1a) After running the experiment a total of 12 times, we get the following output:

{tasks}      [ (STACK) ./a.out {tasks} 400000000 ]      [ (DARRAY) ./a.out {tasks} 400000000 ]
1                 1.41u 0.00s 0:01.41r 5056kb                 6.41u 0.02s 0:06.41r 4892kb
2                 2.83u 0.00s 0:01.41r 5252kb                 12.82u 0.02s 0:06.41r 5064kb
4                 5.69u 0.01s 0:01.43r 5292kb                 26.08u 0.05s 0:06.83r 5304kb

{tasks}      [ (VECTOR1) ./a.out {tasks} 400000000 ]      [ (VECTOR2) ./a.out {tasks} 100000000 ]
1                 8.22u 0.03s 0:08.22r 5056kb                 9.86u 0.04s 0:09.86r 4904kb
2                 16.46u 0.01s 0:08.24r 5120kb                19.63u 0.02s 0:09.81r 4948kb
4                 32.65u 0.04s 0:08.20r 5200kb                39.33u 0.06s 0:09.86r 5244kb

1b) Across all tasks we see that real time remains almost exactly the same across all the tests, whereas the user
time seems to double with the number of tasks. To be more specific, the user time seems to increase linearly with
the number of tasks and the real time seems not to scale at all.

1c)
    STACK: This implementation uses a fixed size array which allows us to query values very quickly. This explains
        why each thread executes so quickly.

    DARRAY: This implementation uses a unique pointer to an array, this means that accessing values in the array will
        take more time than the previous approach. This explains the 6x relative increase in user time for the critical
        path.

    VECTOR1: This implementation uses a dynamically created vector, we then query its indexes for a value. This is
        slower than the DARRAY as a pointer to a static array is a bit faster in creation. This explains why it's slower
        than the previous 2.

    VECTOR2: This implementation is slower than VECTOR1 as instead of accessing elements, we are pushing elements to the
        back of the vector which is more computationally intensive. Thus explaining why its slower than the previous 3.